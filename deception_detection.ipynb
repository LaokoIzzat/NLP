{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     E:\\Users\\laoko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     E:\\Users\\laoko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     E:\\Users\\laoko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# These are required downloads for nltk stopwords, tokenizer and lemmatizer.\n",
    "# If you are running this code more than once, you can make this \"if False\" so that you do not download this data again.\n",
    "if True:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"DOC_ID\":  # skip the header\n",
    "                continue\n",
    "            # Here and in splitData function, I have added the three other categories of features\n",
    "            # to be parsed in for Q5.\n",
    "            (Id, Text, Label, Verified_Purchase, Product_Category, Review_Title) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label, Verified_Purchase, Product_Category, Review_Title))\n",
    "# Here I have fed in more features for review by unpacking more variables in the tuple that parseReview returns.\n",
    "# I have also made sure to implement these changes below in the splitData function.\n",
    "\n",
    "def splitData(percentage):\n",
    "    # A method to split the data between trainData and testData \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (Id, Text, Label, Verified_Purchase, Rating, Product_title) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text, Verified_Purchase, Rating, Product_title)),Label))\n",
    "    for (Id, Text, Label, Verified_Purchase, Rating, Product_title) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text, Verified_Purchase, Rating, Product_title)),Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional features added variations and results\n",
    "I tried all of the other features for each document in the reviews text file including rating,\n",
    "verified purchase, product category, product ID, product title, review title. I tested these individually alongside the initial 3 features of the review that we were testing. The results I obtained were as follows:\n",
    "1. (ID, Review Text, Label) + Verified Purchase resulted in an average accuracy score of 81.2% \n",
    "2. (ID, Review Text, Label) + Rating resulted in an average accuracy score of 66.6% \n",
    "3. (ID, Review Text, Label) + Product Category resulted in an average accuracy score of 65.7% \n",
    "4. (ID, Review Text, Label) + Product ID resulted in an average accuracy score of 65.3% \n",
    "5. (ID, Review Text, Label) + Product Title resulted in an average accuracy score of 65.9% \n",
    "6. (ID, Review Text, Label) + Review Title resulted in an average accuracy score of 65.6% \n",
    "\n",
    "When combining three additional features together, I found that the score with Verified Purchase, Rating, and Product Title gave the best average accuracy score of 82.5%. \n",
    "\n",
    "However, some other combinations I tried and recorded the results for included:\n",
    "1. (ID, Review Text, Label) + Verified Purchase + Rating + Review Title resulted in an average accuracy score of 81.4%.\n",
    "2. (ID, Review Text, Label) + Verified Purchase + Rating + Product ID resulted in an average accuracy score of 81.2%.\n",
    "3. (ID, Review Text, Label) + Verified Purchase + Rating + Product Category resulted in an average accuracy score of 81.3%.\n",
    "3. (ID, Review Text, Label) + Verified Purchase + Product Title + Review Title resulted in an average accuracy score of 82.5%. (This is the same as the score I obtained when using Verified Purchase, Rating, and Product Title however, I chose to keep the latter in the final code submission as I found I had less fluctuation in the results I was achieving when running Cross Validate multiple times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Returns a tuple of 6 including: an integer (DOC_ID), a string containing the review (REVIEW_TEXT),\n",
    "    # a string indicating the label (LABEL), a string indicating whether the purchase was verified for the review (VERIFIED_PURCHASE),\n",
    "    # a string of a number between 1-5 indicating the rating (RATING), and a string indicating the product title (PRODUCT_TITLE).   \n",
    "    \n",
    "    # Here I am using an if condition to change __label1__ to fake and __label2__ to real,\n",
    "    # for the label in the triple.\n",
    "    if reviewLine[1] == \"__label1__\":\n",
    "        reviewLine[1] = fakeLabel\n",
    "    else:\n",
    "        reviewLine[1] = realLabel\n",
    "    return (int(reviewLine[0]), reviewLine[8], reviewLine[1], reviewLine[3], str(reviewLine[2]), reviewLine[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing variations and results\n",
    "All tests were carried out over 5 runs of the cross validate function and recorded average accuracy. N.B. I carried these tests out before adding the extra features like rating etc so that I could see the effect of these changes in pre processing on the actual review text itself.\n",
    "1. word tokenize alone yielded accuracy score of 65.11%.\n",
    "2. word tokenize + lemmatization + removing all punctuation yielded 64.71% accuracy.\n",
    "3. word tokenize + punctuation removed gave 64.5% accuracy.\n",
    "4. word tokenize + lemmatization gave 64.9% accuracy.\n",
    "5. word tokenize + stop word removal yielded 64.8% accuracy.\n",
    "6. word tokenize + porter stemmer gave the best accuracy of 65.2% on average and therefore, I stuck with this as my final preprocessing function.\n",
    "\n",
    "My methods of implementing lemmatization, stop word removal and punctuation removal will be left in a comment block under preProcess function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input: a string of one review\n",
    "def preProcess(text, Verified_Purchase, Rating, Product_title):\n",
    "    # The code here is tokenizing a string to split off punctuation other than periods.\n",
    "    # I have also used WordNetLemmatizer from NLTK to lemmatize for nouns and verbs. \n",
    "    # Also some simple normalisation by lowercasing all words in the list as well as removing all punctuation\n",
    "    # of tokens by using list comprehension.\n",
    "    \n",
    "    # New text is the review text from parse review with verified purchase, product category, and \n",
    "    # review title added to it so that these can also become features of the text and will also have a\n",
    "    # weight when they become feature vectors in toFeatureVector.\n",
    "    new_text = text + \" \" + Verified_Purchase + \" \" + Rating +  \" \" + Product_title + \" \"\n",
    "    tokens = word_tokenize(new_text)\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    tokens = [token.lower() for token in tokens] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     lemma = WordNetLemmatizer()\n",
    "#     tokens = [lemma.lemmatize(t, pos = \"v\") for t in tokens] # Lemmatizing for verbs.\n",
    "#     tokens = [lemma.lemmatize(t, pos = \"n\") for t in tokens] # Lemmatizing for nouns.\n",
    "#     stop_words = set(stopwords.words('english')) \n",
    "#     tokens = [token for token in tokens if not token in stop_words]\n",
    "#     tokens = [token for token in tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    featureVec = {}\n",
    "\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            featureVec[w] += 1.0/len(tokens)\n",
    "        except KeyError:\n",
    "            featureVec[w] = 1.0/len(tokens)\n",
    "        try:\n",
    "            featureDict[w] += 1.0/len(tokens)\n",
    "        except KeyError:\n",
    "            featureDict[w] = 1.0/len(tokens)\n",
    "    \n",
    "    # Using bigrams did not improve the results in any combonation of features.\n",
    "    if False:\n",
    "        # just get bigram binary presence or not\n",
    "        for i in range(1, len(tokens)):\n",
    "            bigram = tokens[i-1] + \" \" + tokens[i]\n",
    "            try:\n",
    "                featureVec[bigram] = 1 #+= 1.0/len(tokens)\n",
    "            except KeyError:\n",
    "                featureVec[bigram] = 1 #= 1.0/len(tokens)\n",
    "            try:\n",
    "                featureDict[bigram] += 1.0\n",
    "            except KeyError:\n",
    "                featureDict[bigram] = 1.0\n",
    "                \n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM parameter tuning variations and results \n",
    "Here I tried changing the regularization parameter C of the svm from 1.0 (default) to 0.9, 0.8 and so on. In theory a lower C should produce a larger margin-seperating hyperplane however, I did not find this to be useful for this particular datasetand didn't get any better results than when it was left at default. My results were as follows:\n",
    "1. C = 0.9 yielded accuracy score of 64.9%\n",
    "2. C = 0.8 yielded accuracy score of 64.8%\n",
    "3. C = 0.7 yielded accuracy score of 64.8%\n",
    "4. C = 0.6 yielded accuracy score of 64.5%\n",
    "5. C = 0.5 yielded accuracy score of 64.3%\n",
    "6. C = 0.4 yielded accuracy score of 64.1%\n",
    "7. C = 0.3 yielded accuracy score of 63.8%\n",
    "8. C = 0.2 yielded accuracy score of 63.3%\n",
    "9. C = 0.1 yielded accuracy score of 62.5%\n",
    "\n",
    "I have also made the tolerance smaller to 1e-5 from 1e-4 (the default) which gave an average accuracy score of 65.2% too however, there was less fluctuation in the results I obtained when using a smaller tolerance compared to the default. This was the opposite for when I made the tolerance larger (1e-3) as the results were also around the 65.2% mark on average however, there was more fluctuation in between runs of the crossValidate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(tol = 1e-5))]) \n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    \n",
    "    for i in range(0,len(dataset),int(foldSize)):\n",
    "        print(\"Fold start on items %d - %d\" % (i, i+foldSize))\n",
    "        myTestData = dataset[i:i+foldSize]\n",
    "        myTrainData = dataset[:i] + dataset[i+foldSize:]\n",
    "        classifier = trainClassifier(myTrainData)\n",
    "        y_true = [x[1] for x in myTestData]\n",
    "        y_pred = predictLabels(myTestData, classifier)\n",
    "        print(len(myTestData))\n",
    "        results.append(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
    "        \n",
    "    print(zip(*results))\n",
    "    avgResults = [np.mean([x[0] for x in results]),\n",
    "                   np.mean([x[1] for x in results]),\n",
    "                   np.mean([x[2] for x in results])\n",
    "                ]\n",
    "    return avgResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "53255\n",
      "Fold start on items 0 - 1680\n",
      "Training Classifier...\n",
      "1680\n",
      "Fold start on items 1680 - 3360\n",
      "Training Classifier...\n",
      "1680\n",
      "Fold start on items 3360 - 5040\n",
      "Training Classifier...\n",
      "1680\n",
      "Fold start on items 5040 - 6720\n",
      "Training Classifier...\n",
      "1680\n",
      "Fold start on items 6720 - 8400\n",
      "Training Classifier...\n",
      "1680\n",
      "Fold start on items 8400 - 10080\n",
      "Training Classifier...\n",
      "1680\n",
      "Fold start on items 10080 - 11760\n",
      "Training Classifier...\n",
      "1680\n",
      "Fold start on items 11760 - 13440\n",
      "Training Classifier...\n",
      "1680\n",
      "Fold start on items 13440 - 15120\n",
      "Training Classifier...\n",
      "1680\n",
      "Fold start on items 15120 - 16800\n",
      "Training Classifier...\n",
      "1680\n",
      "<zip object at 0x0000017DF0D94180>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8273395937613269, 0.8239285714285713, 0.8234925000491996]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize global lists that will be appended to by the methods below\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# We do the cross validation on the 80% (training data)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "splitData(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "crossValidate(trainData, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Done training!\n",
      "Precision: 0.821190\n",
      "Recall: 0.816190\n",
      "F Score:0.815472\n"
     ]
    }
   ],
   "source": [
    "functions_complete = True\n",
    "if functions_complete:\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
